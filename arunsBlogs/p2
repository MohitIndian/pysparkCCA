### grp

*****************
1. sqoop import
*****************

sqoop eval \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--query "select count(*) from products"

------------------------
| count(*)             | 
------------------------
| 1345                 | 
------------------------

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products \
--fields-terminated-by "|" \
--as-textfile \
--target-dir /user/cloudera/products \
--delete-target-dir

# make sure counts match

hadoop fs -ls /user/cloudera/products
hadoop fs -cat /user/cloudera/products/* | wc -l
1345

*****************
2. hdfs
*****************

hadoop fs -mv /user/cloudera/products /user/cloudera/problem2

*****************
3. hdfs
*****************

read = 4, write = 2, execute = 1
owner: read/write/execute = 4+2+1=7
group: read/write = 4+2=6
others: read/execute = 4+1=5

hadoop fs -chmod 765 /user/cloudera/problem2/products/*

*****************
4a. spark schema
*****************

pyspark --master yarn \
--packages com.databricks:spark-csv_2.10:1.5.0,com.databricks:spark-avro_2.10:4.0.0

import rlcompleter, readline
readline.parse_and_bind("tab: complete")
sqlContext.setConf("spark.sql.shuffle.partitions", "8")
from pyspark.sql.functions import *
from pyspark.sql.types import *

schema = StructType([ \
StructField("product_id", IntegerType(), True), \
StructField("product_category_id", IntegerType(), True), \
StructField("product_name", StringType(), True), \
StructField("product_desc", StringType(), True), \
StructField("product_price", FloatType(), True), \
StructField("product_image", StringType(), True)])

p2DF = sqlContext\
.read.\
format("com.databricks.spark.csv")\
.option("header", "false")\
.option("delimiter", "|")\
.schema(schema)\
.load("/user/cloudera/problem2/products")

p2DF.printSchema()
root
 |-- product_id: integer (nullable = true)
 |-- product_category_id: integer (nullable = true)
 |-- product_name: string (nullable = true)
 |-- product_desc: string (nullable = true)
 |-- product_price: float (nullable = true)
 |-- product_image: string (nullable = true)

p2DF.count()
1345 

or

from pyspark.sql import Row
p2RDD = sc.textFile("/user/cloudera/problem2/products")

p2DF_V2 = p2RDD\
.map(lambda x: x.split("|"))\
.map(lambda x: Row(\
int(x[0]),\
int(x[1]),\
x[2],\
x[3],\
float(x[4]),\
x[5]))\
.toDF(["product_id", \
"product_category_id", \
"product_name", \
"product_desc", \
"product_price", \
"product_image"])

p2DF_V2.printSchema()
root
 |-- product_id: long (nullable = true)
 |-- product_category_id: long (nullable = true)
 |-- product_name: string (nullable = true)
 |-- product_desc: string (nullable = true)
 |-- product_price: double (nullable = true)
 |-- product_image: string (nullable = true)

p2DF_V2.count()
1345

*****************
4b. spark sql
*****************  

p2DF.registerTempTable("products")

finalDF = sqlContext.sql \
("""
select product_category_id, 
count(product_id) as count, 
max(product_price) as max, 
round(avg(product_price), 2) as avg, 
min(product_price) as min 
from products 
where product_price < 100 
group by product_category_id 
order by product_category_id
""")

finalDF.count()
55

finalDF.show(10)
+-------------------+-----+-----+-----+-----+                                   
|product_category_id|count|  max|  avg|  min|
+-------------------+-----+-----+-----+-----+
|                  2|   11|99.99|66.81|29.97|
|                  3|   19| 99.0|55.73|  0.0|
|                  4|   10|99.95|55.89|21.99|
|                  5|   13|99.99|57.99| 14.0|
|                  6|   19|99.99|43.94| 14.0|
|                  7|   18|99.98|47.49| 14.0|
|                  8|   19|99.98|41.67|21.99|
|                  9|   17|99.99|67.17| 28.0|
|                 10|    4|99.95|78.48|34.99|
|                 11|    5|99.99|76.99|34.99|
+-------------------+-----+-----+-----+-----+

or

apiDF = p2DF\
.filter("product_price < 100")\
.groupBy("product_category_id")\
.agg(count("product_id").alias("count"),\
max("product_price").alias("max"),\
round(avg("product_price"), 2).alias("avg"),\
min("product_price").alias("min"))\
.orderBy("product_category_id")

apiDF.count()
55

apiDF.show(10)
+-------------------+-----+-----+-----+-----+
|product_category_id|count|  max|  avg|  min|
+-------------------+-----+-----+-----+-----+
|                  2|   11|99.99|66.81|29.97|
|                  3|   19| 99.0|55.73|  0.0|
|                  4|   10|99.95|55.89|21.99|
|                  5|   13|99.99|57.99| 14.0|
|                  6|   19|99.99|43.94| 14.0|
|                  7|   18|99.98|47.49| 14.0|
|                  8|   19|99.98|41.67|21.99|
|                  9|   17|99.99|67.17| 28.0|
|                 10|    4|99.95|78.48|34.99|
|                 11|    5|99.99|76.99|34.99|
+-------------------+-----+-----+-----+-----+

sqlContext.sql("select count(distinct(product_category_id)) from products").show()
+---+
|_c0|
+---+
| 55|
+---+

*****************************
5 . spark write / compression (avro / snappy)
*****************************

sqlContext.setConf("spark.sql.avro.compression.codec", "snappy")

finalDF\
.write\
.mode("overwrite")\
.format("com.databricks.spark.avro")\
.option("codec", "snappy")\ #alternative method to setting conf
.save("/user/cloudera/problem2/avroSnappy")

hadoop fs -ls /user/cloudera/problem2/avroSnappy
# file(s) will have *.avro
# confirm with sample file
avro-tools getmeta hdfs://quickstart.cloudera:8020/user/cloudera/problem2/avroSnappy/<sampleFile>
or
hadoop fs -cat /user/cloudera/problem2/avroSnappy/<sampleFile> | head
# codec will be snappy like this - avro.codec snappy

### grp
